Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/jupyter_cache/executors/utils.py", line 51, in single_nb_execution
    executenb(
  File "/usr/local/lib/python3.9/site-packages/nbclient/client.py", line 1085, in execute
    return NotebookClient(nb=nb, resources=resources, km=km, **kwargs).execute()
  File "/usr/local/lib/python3.9/site-packages/nbclient/util.py", line 84, in wrapped
    return just_run(coro(*args, **kwargs))
  File "/usr/local/lib/python3.9/site-packages/nbclient/util.py", line 62, in just_run
    return loop.run_until_complete(coro)
  File "/usr/local/lib/python3.9/asyncio/base_events.py", line 642, in run_until_complete
    return future.result()
  File "/usr/local/lib/python3.9/site-packages/nbclient/client.py", line 551, in async_execute
    await self.async_execute_cell(
  File "/usr/local/lib/python3.9/site-packages/nbclient/client.py", line 846, in async_execute_cell
    self._check_raise_for_error(cell, exec_reply)
  File "/usr/local/lib/python3.9/site-packages/nbclient/client.py", line 748, in _check_raise_for_error
    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
------------------
from vowpalwabbit import pyvw

# the label for each word is its parent, or -1 for root
my_dataset = [ [("the",      1),   # 0
                ("monster",  2),   # 1
                ("ate",     -1),   # 2
                ("a",        5),   # 3
                ("big",      5),   # 4
                ("sandwich", 2)]  # 5
                ,
               [("the",      1),   # 0
                ("sandwich", 2),   # 1
                ("is",      -1),   # 2
                ("tasty",    2)]   # 3
                ,
               [("a",        1),   # 0
                ("sandwich", 2),   # 1
                ("ate",     -1),   # 2
                ("itself",   2)]   # 3
                ]

class CovingtonDepParser(pyvw.SearchTask):
    def __init__(self, vw, sch, num_actions):
        pyvw.SearchTask.__init__(self, vw, sch, num_actions)
        sch.set_options( sch.AUTO_HAMMING_LOSS | sch.AUTO_CONDITION_FEATURES )

    def _run(self, sentence):
        N = len(sentence)
        # initialize our output so everything is a root
        output = [-1 for i in range(N)]
        for n in range(N):
            wordN,parN = sentence[n]
            for m in range(-1,N):
                if m == n: continue
                wordM = sentence[m][0] if m > 0 else "*root*"
                # ask the question: is m the parent of n?
                isParent = 2 if m == parN else 1

                # construct an example
                dir = 'l' if m < n else 'r'
                ex = self.vw.example({'a': [wordN, dir + '_' + wordN],
                                      'b': [wordM, dir + '_' + wordN],
                                      'p': [wordN + '_' + wordM, dir + '_' + wordN + '_' + wordM],
                                      'd': [ str(m-n <= d) + '<=' + str(d) for d in [-8, -4, -2, -1, 1, 2, 4, 8] ] +
                                           [ str(m-n >= d) + '>=' + str(d) for d in [-8, -4, -2, -1, 1, 2, 4, 8] ] })
                pred = self.sch.predict(examples  = ex,
                                        my_tag    = (m+1)*N + n + 1,
                                        oracle    = isParent,
                                        condition = [ (max(0, (m  )*N + n + 1), 'p'),
                                                      (max(0, (m+1)*N + n    ), 'q') ])
                vw.finish_example([ex]) # must pass the example in as a list because search is a MultiEx reduction
                if pred == 2:
                    output[n] = m
                    break
        return output

class CovingtonDepParserLDF(pyvw.SearchTask):
    def __init__(self, vw, sch, num_actions):
        pyvw.SearchTask.__init__(self, vw, sch, num_actions)
        sch.set_options( sch.AUTO_HAMMING_LOSS | sch.IS_LDF | sch.AUTO_CONDITION_FEATURES )

    def makeExample(self, sentence, n, m):
        wordN = sentence[n][0]
        wordM = sentence[m][0] if m >= 0 else '*ROOT*'
        dir   = 'l' if m < n else 'r'
        ex = self.vw.example( { 'a': [wordN, dir + '_' + wordN],
                                'b': [wordM, dir + '_' + wordM],
                                'p': [wordN + '_' + wordM, dir + '_' + wordN + '_' + wordM],
                                'd': [ str(m-n <= d) + '<=' + str(d) for d in [-8, -4, -2, -1, 1, 2, 4, 8] ] +
                                     [ str(m-n >= d) + '>=' + str(d) for d in [-8, -4, -2, -1, 1, 2, 4, 8] ] },
                              labelType=self.vw.lCostSensitive)
        # the label string is (m+2):0. The :0 means cost zero (this is
        # irrelevant and could be any number). +2 ensures >= 1
        ex.set_label_string(str(100 + n - m) + ":0")
        return ex

    def _run(self, sentence):
        N = len(sentence)
        # initialize our output so everything is a root
        output = [-1 for i in range(N)]
        for n in range(N):
            # make LDF examples
            examples = []
            for m in range(-1, N):
                if n != m:
                    examples.append(self.makeExample(sentence=sentence, n=n, m=m))

            # truth
            parN = sentence[n][1]

            # Mapping:
            # -1      => 1
            # 0...n-1 => 2...n+1
            # n+1...N => n+2 ...N+1
            oracle = parN+2 if parN < n else parN + 1   # have to -1 because we excluded n==m from list

            # make a prediction
            pred = self.sch.predict(examples  = examples,
                                    my_tag    = n+1,
                                    oracle    = oracle,
                                    condition = [ (n, 'p'), (n-1, 'q') ] )

            vw.finish_example(examples)

            # Reverse mapping:
            # 1 => -1
            # 2...n+1 => 0...n-1
            # n+2...N+1 => n+1...N
            output[n] = pred-2 if pred <= n + 1 else pred - 1 # have to +1 because n==m excluded

        return output

# TODO: if they make sure search=0 <==> ldf <==> csoaa_ldf

# demo the non-ldf version:

print('training non-LDF')
vw = pyvw.Workspace("--search 2 --search_task hook --ring_size 1024 --quiet")
task = vw.init_search_task(CovingtonDepParser)
for p in range(2): # do two passes over the training data
    task.learn(my_dataset)
print('testing non-LDF')
print(task.predict( [(w,-1) for w in "the monster ate a sandwich".split()] ))
print('should have printed [ 1 2 -1 4 2 ]')

print('training LDF')
vw = pyvw.Workspace("--search 0 --csoaa_ldf m --search_task hook --ring_size 1024 --quiet")
task = vw.init_search_task(CovingtonDepParserLDF)
for p in range(100): # do two passes over the training data
   task.learn(my_dataset)
print('testing LDF')
print(task.predict( [(w,-1) for w in "the monster ate a sandwich".split()] ))
print('should have printed [ 1 2 -1 4 2 ]')

------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mRuntimeError[0m                              Traceback (most recent call last)
Input [0;32mIn [1][0m, in [0;36m<module>[0;34m[0m
[1;32m    124[0m [38;5;28mprint[39m([38;5;124m'[39m[38;5;124mshould have printed [ 1 2 -1 4 2 ][39m[38;5;124m'[39m)
[1;32m    126[0m [38;5;28mprint[39m([38;5;124m'[39m[38;5;124mtraining LDF[39m[38;5;124m'[39m)
[0;32m--> 127[0m vw [38;5;241m=[39m [43mpyvw[49m[38;5;241;43m.[39;49m[43mWorkspace[49m[43m([49m[38;5;124;43m"[39;49m[38;5;124;43m--search 0 --csoaa_ldf m --search_task hook --ring_size 1024 --quiet[39;49m[38;5;124;43m"[39;49m[43m)[49m
[1;32m    128[0m task [38;5;241m=[39m vw[38;5;241m.[39minit_search_task(CovingtonDepParserLDF)
[1;32m    129[0m [38;5;28;01mfor[39;00m p [38;5;129;01min[39;00m [38;5;28mrange[39m([38;5;241m100[39m): [38;5;66;03m# do two passes over the training data[39;00m

File [0;32m/usr/local/lib/python3.9/site-packages/vowpalwabbit/pyvw.py:451[0m, in [0;36mWorkspace.__init__[0;34m(self, arg_str, enable_logging, **kw)[0m
[1;32m    449[0m     [38;5;28msuper[39m()[38;5;241m.[39m[38;5;21m__init__[39m([38;5;124m"[39m[38;5;124m [39m[38;5;124m"[39m[38;5;241m.[39mjoin(l), [38;5;28mself[39m[38;5;241m.[39m_log_wrapper)
[1;32m    450[0m [38;5;28;01melse[39;00m:
[0;32m--> 451[0m     [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[38;5;241;43m.[39;49m[38;5;21;43m__init__[39;49m[43m([49m[38;5;124;43m"[39;49m[38;5;124;43m [39;49m[38;5;124;43m"[39;49m[38;5;241;43m.[39;49m[43mjoin[49m[43m([49m[43ml[49m[43m)[49m[43m)[49m
[1;32m    452[0m [38;5;28mself[39m[38;5;241m.[39minit [38;5;241m=[39m [38;5;28;01mTrue[39;00m
[1;32m    454[0m [38;5;66;03m# check to see if native parser needs to run[39;00m

[0;31mRuntimeError[0m: Tried to use a multiline reduction as a singleline reduction. Name: shared_feature_merger
RuntimeError: Tried to use a multiline reduction as a singleline reduction. Name: shared_feature_merger

